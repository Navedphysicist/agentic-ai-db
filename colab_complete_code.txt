# Agentic AI DB - Automatic Multi-Source Ingestion (Google Colab)
# Copy each section into separate cells in Google Colab

# =============================================
# CELL 1: Title and Description
# =============================================
"""
# Agentic AI DB - Automatic Multi-Source Ingestion Agent

The RIGHT way to build data ingestion: Upload files → Agent discovers everything → Ask questions later!

## Correct Workflow
1. 📁 User uploads ANY data file (CSV, SQLite, JSON, MongoDB dump)
2. 🤖 Agent auto-detects file type and discovers ALL tables/collections
3. 📊 Agent automatically reads ALL available data
4. 💬 User can then ask natural language questions
5. 🧠 Other agents (Planner, Executor, Summarizer) handle the questions

## What the Ingestion Agent Does
✅ Auto-detects file types (CSV, SQLite, JSON, etc.)
✅ Discovers all tables in databases automatically
✅ Discovers all collections in JSON/MongoDB files
✅ Reads ALL data without requiring user queries
✅ Creates unified DataFrames ready for analysis
✅ Generates unique dataset IDs for tracking

## Why No User Queries?
- Users shouldn't write SQL - that's technical!
- Natural language questions come later with other agents
- Ingestion = get ALL the data first, ask questions later
- This matches real-world data platform behavior

## Learning Goals
1. Understand automatic data discovery
2. Learn the upload-only user experience
3. See how agents prepare data for natural language queries
4. Master the complete data ingestion workflow
"""

# =============================================
# CELL 2: Install Dependencies
# =============================================
!pip install pandas pyarrow sqlalchemy

# =============================================
# CELL 3: Import Libraries
# =============================================
import pandas as pd
import sqlite3
import uuid
from pathlib import Path
import os
import json

# =============================================
# CELL 4: Create Sample Data Files
# =============================================
print("Creating sample data files for automatic ingestion...")

# 1. CSV Data (simple upload)
csv_data = {
    'order_date': ['2024-01-15', '2024-01-20', '2024-02-01', '2024-02-15', '2024-03-01'],
    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],
    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],
    'quantity': [2, 5, 3, 1, 4],
    'unit_price': [1200.00, 25.99, 89.99, 299.99, 79.99],
    'customer_region': ['North', 'South', 'East', 'West', 'North']
}

df_csv = pd.DataFrame(csv_data)
df_csv.to_csv('sample_sales.csv', index=False)
print("✅ CSV file created - user can just upload this!")

# 2. SQLite Database (multiple tables - agent will discover all)
products_data = {
    'product_id': [1, 2, 3, 4, 5],
    'product_name': ['Laptop Pro', 'Wireless Mouse', 'USB Keyboard', '4K Monitor', 'Bluetooth Headphones'],
    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],
    'price': [1299.99, 29.99, 79.99, 399.99, 159.99],
    'stock_quantity': [15, 50, 30, 8, 25]
}

sales_data = {
    'sale_id': [1, 2, 3, 4, 5],
    'product_id': [1, 2, 1, 3, 4],
    'quantity_sold': [2, 5, 1, 3, 1],
    'sale_date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],
    'total_amount': [2599.98, 149.95, 1299.99, 239.97, 399.99]
}

# Create SQLite with multiple tables
conn = sqlite3.connect('sample_inventory.db')
products_df = pd.DataFrame(products_data)
sales_df = pd.DataFrame(sales_data)
products_df.to_sql('products', conn, index=False, if_exists='replace')
sales_df.to_sql('sales', conn, index=False, if_exists='replace')
conn.close()
print("✅ SQLite database created with MULTIPLE tables - agent will find them all!")

# 3. JSON data (MongoDB-style documents)
customers_data = [
    {"customer_id": 1, "name": "John Smith", "region": "North", "total_orders": 5, "lifetime_value": 2500.99},
    {"customer_id": 2, "name": "Sarah Johnson", "region": "South", "total_orders": 3, "lifetime_value": 1850.50},
    {"customer_id": 3, "name": "Mike Davis", "region": "East", "total_orders": 7, "lifetime_value": 3200.75},
    {"customer_id": 4, "name": "Emily Wilson", "region": "West", "total_orders": 2, "lifetime_value": 999.99}
]

with open('sample_customers.json', 'w') as f:
    json.dump(customers_data, f, indent=2)
print("✅ JSON file created - agent will read all documents automatically!")

print(f"\n📊 Sample files summary:")
print(f"CSV: {len(df_csv)} rows")
print(f"SQLite: {len(products_df)} products + {len(sales_df)} sales")
print(f"JSON: {len(customers_data)} customers")
print("\n💡 User just uploads these files - agent does the rest!")

# =============================================
# CELL 5: State Management
# =============================================
def create_initial_state():
    return {
        "status": "initialized",
        "source_type": None,
        "dataset_id": None,
        "df": None,
        "schema": None,
        "error": None
    }

def update_state(state, **updates):
    state.update(updates)
    return state

def get_status_summary(state):
    if state.get('error'):
        return f"Error: {state['error']}"

    status_parts = []
    
    if state.get('status'):
        status_parts.append(f"Status: {state['status']}")
    if state.get('source_type'):
        status_parts.append(f"Source: {state['source_type']}")
    if state.get('dataset_id'):
        status_parts.append(f"Dataset: {state['dataset_id']}")
    if state.get('df') is not None:
        status_parts.append(f"Data: {state['df'].shape[0]} rows")

    return " | ".join(status_parts) if status_parts else "No data loaded"

print("✅ State management ready!")

# =============================================
# CELL 6: CSV Handler (Simple Upload)
# =============================================
class CSVHandler:

    def process_csv(self, file_path, max_rows=10000):
        try:
            df = pd.read_csv(file_path)

            if len(df) > max_rows:
                print(f"Large dataset detected. Using first {max_rows} rows.")
                df = df.head(max_rows)

            schema = self._get_basic_schema(df)
            return df, schema
            
        except Exception as e:
            raise ValueError(f"Failed to process CSV file: {str(e)}")

    def _get_basic_schema(self, df):
        schema = {
            "columns": list(df.columns),
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "data_types": {col: str(df[col].dtype) for col in df.columns}
        }
        return schema

    def validate_csv_file(self, file_path):
        try:
            if not Path(file_path).exists():
                return False, "File does not exist"
            if not file_path.lower().endswith('.csv'):
                return False, "File must be a CSV"
            if Path(file_path).stat().st_size == 0:
                return False, "File is empty"
            return True, "File is valid"
        except Exception as e:
            return False, f"Validation error: {str(e)}"

    def generate_dataset_id(self, file_path):
        filename = Path(file_path).stem
        short_uuid = str(uuid.uuid4())[:8]
        return f"csv_{filename}_{short_uuid}"

print("✅ CSV Handler ready - just upload and go!")

# =============================================
# CELL 7: SQLite Handler (Auto-Discovery)
# =============================================
class SQLiteHandler:

    def process_sqlite_file(self, db_path, max_rows=10000):
        """Automatically discovers and reads ALL tables in SQLite database"""
        try:
            conn = sqlite3.connect(db_path)
            
            # Discover all tables automatically
            tables_query = "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
            tables_df = pd.read_sql_query(tables_query, conn)
            
            if tables_df.empty:
                raise ValueError("No tables found in SQLite database")
            
            table_names = tables_df['name'].tolist()
            print(f"🔍 Auto-discovered {len(table_names)} tables: {table_names}")
            
            all_data = {}
            
            # Read ALL tables automatically
            for table_name in table_names:
                query = f"SELECT * FROM {table_name}"
                df = pd.read_sql_query(query, conn)
                
                if len(df) > max_rows:
                    print(f"Large table '{table_name}' detected. Using first {max_rows} rows.")
                    df = df.head(max_rows)
                
                df = df.add_prefix(f"{table_name}_")
                all_data[table_name] = df
                print(f"   📊 {table_name}: {len(df)} rows, {len(df.columns)} columns")
            
            # Combine all tables
            combined_df = self._combine_tables(all_data)
            schema = self._get_basic_schema(combined_df, table_names)
            conn.close()
            
            print(f"✅ Combined all tables: {len(combined_df)} rows, {len(combined_df.columns)} columns")
            return combined_df, schema
            
        except Exception as e:
            raise ValueError(f"Failed to process SQLite file: {str(e)}")

    def _combine_tables(self, all_data):
        # Create summary + include data from largest table
        summary_data = []
        for table_name, df in all_data.items():
            summary_row = {
                'table_name': table_name,
                'row_count': len(df),
                'columns': ', '.join(df.columns[:3])
            }
            summary_data.append(summary_row)
        
        summary_df = pd.DataFrame(summary_data)
        largest_table = max(all_data.items(), key=lambda x: len(x[1]))
        largest_df = largest_table[1].head(50)  # Include actual data
        
        # Combine summary with actual data
        combined_df = pd.concat([summary_df.add_prefix('meta_'), largest_df], axis=1, sort=False)
        return combined_df

    def _get_basic_schema(self, df, table_names):
        schema = {
            "columns": list(df.columns),
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "data_types": {col: str(df[col].dtype) for col in df.columns},
            "tables_discovered": table_names,
            "source_info": f"Auto-discovered and combined {len(table_names)} tables"
        }
        return schema

    def generate_dataset_id(self, db_path):
        filename = Path(db_path).stem
        short_uuid = str(uuid.uuid4())[:8]
        return f"sqlite_{filename}_{short_uuid}"

print("✅ SQLite Handler ready - auto-discovers all tables!")

# =============================================
# CELL 8: JSON Handler (Document Discovery)
# =============================================
class JSONHandler:

    def process_json_file(self, file_path, max_rows=10000):
        """Automatically reads all documents from JSON file"""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            # Ensure data is a list of documents
            if isinstance(data, dict):
                data = [data]
            elif not isinstance(data, list):
                raise ValueError("JSON file must contain an object or array")
            
            if len(data) > max_rows:
                print(f"Large dataset detected. Using first {max_rows} documents.")
                data = data[:max_rows]
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            schema = self._get_basic_schema(df)
            
            print(f"✅ JSON processed: {len(df)} documents, {len(df.columns)} fields")
            return df, schema
            
        except Exception as e:
            raise ValueError(f"Failed to process JSON file: {str(e)}")

    def _get_basic_schema(self, df):
        schema = {
            "columns": list(df.columns),
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "data_types": {col: str(df[col].dtype) for col in df.columns},
            "source_info": "JSON documents automatically processed"
        }
        return schema

    def generate_dataset_id(self, file_path):
        filename = Path(file_path).stem
        short_uuid = str(uuid.uuid4())[:8]
        return f"json_{filename}_{short_uuid}"

print("✅ JSON Handler ready - reads all documents automatically!")

# =============================================
# CELL 9: Automatic Ingestion Agent
# =============================================
def detect_data_source(file_path):
    """Auto-detect file type"""
    if file_path.lower().endswith('.csv'):
        return "csv"
    elif file_path.lower().endswith(('.db', '.sqlite', '.sqlite3')):
        return "sqlite"
    elif file_path.lower().endswith(('.json', '.jsonl')):
        return "json"
    return "unknown"

def ingest_data_file(file_path, state):
    """Universal file ingestion - just upload and go!"""
    print(f"🔍 Auto-detecting file type: {file_path}")
    
    source_type = detect_data_source(file_path)
    print(f"📋 Detected: {source_type}")
    
    state = update_state(state, status="processing")
    
    try:
        if source_type == "csv":
            handler = CSVHandler()
            df, schema = handler.process_csv(file_path)
            dataset_id = handler.generate_dataset_id(file_path)
            
        elif source_type == "sqlite":
            handler = SQLiteHandler()
            df, schema = handler.process_sqlite_file(file_path)
            dataset_id = handler.generate_dataset_id(file_path)
            
        elif source_type == "json":
            handler = JSONHandler()
            df, schema = handler.process_json_file(file_path)
            dataset_id = handler.generate_dataset_id(file_path)
            
        else:
            raise ValueError(f"Unsupported file type: {source_type}")
        
        state = update_state(
            state, source_type=source_type, dataset_id=dataset_id,
            df=df, schema=schema, status="completed"
        )
        
        print(f"✅ {source_type.upper()} ingested successfully!")
        print(f"📊 Dataset ID: {dataset_id}")
        print(f"📊 Ready for natural language queries!")
        
        return state
        
    except Exception as e:
        state = update_state(state, error=f"{source_type} processing failed: {str(e)}", status="error")
        return state

print("✅ Universal Ingestion Agent ready!")

# =============================================
# CELL 10: Demo - Automatic CSV Ingestion
# =============================================
print("🔄 Demo 1: Automatic CSV Ingestion")
print("=" * 50)
print("User just uploads a CSV file - no queries needed!")

state1 = create_initial_state()
print(f"Initial: {get_status_summary(state1)}")

state1 = ingest_data_file('sample_sales.csv', state1)
print(f"Result: {get_status_summary(state1)}")

if state1.get('df') is not None:
    print("\nSample of ingested data:")
    print(state1['df'].head(3))

# =============================================
# CELL 11: Demo - Automatic SQLite Ingestion
# =============================================
print("\n🔄 Demo 2: Automatic SQLite Ingestion")
print("=" * 50)
print("User uploads SQLite file - agent finds ALL tables!")

state2 = create_initial_state()
print(f"Initial: {get_status_summary(state2)}")

state2 = ingest_data_file('sample_inventory.db', state2)
print(f"Result: {get_status_summary(state2)}")

if state2.get('schema'):
    schema = state2['schema']
    print(f"Tables discovered: {schema.get('tables_discovered', [])}")

# =============================================
# CELL 12: Demo - Automatic JSON Ingestion
# =============================================
print("\n🔄 Demo 3: Automatic JSON Ingestion")
print("=" * 50)
print("User uploads JSON file - agent reads all documents!")

state3 = create_initial_state()
print(f"Initial: {get_status_summary(state3)}")

state3 = ingest_data_file('sample_customers.json', state3)
print(f"Result: {get_status_summary(state3)}")

if state3.get('df') is not None:
    print("\nSample of ingested data:")
    print(state3['df'].head(2))

# =============================================
# CELL 13: What We Learned
# =============================================
print("\n🎓 What We Learned About CORRECT Ingestion Flow")
print("=" * 60)
print("✅ Upload Only: Users just upload files, no technical knowledge needed")
print("✅ Auto-Discovery: Agent automatically finds all tables/collections")  
print("✅ Complete Ingestion: Agent reads ALL available data")
print("✅ No Queries: Users don't write SQL - that comes later!")
print("✅ Natural Language Ready: Data is prepared for human questions")

print("\n🔍 Correct Workflow:")
print("1. 📁 User uploads data files (CSV, SQLite, JSON, etc.)")
print("2. 🤖 Ingestion Agent discovers and reads EVERYTHING")
print("3. 💬 User asks natural language questions like:")
print("   - 'What are the top selling products?'")
print("   - 'Show me revenue by region'")
print("   - 'Which customers spend the most?'")
print("4. 🧠 Planner Agent converts questions to execution plans")
print("5. ⚙️ Executor Agent runs the analysis")
print("6. 📝 Summarizer Agent explains results in plain English")

print("\n💡 Why This Matters:")
print("• Users shouldn't need to know SQL or database schemas")
print("• Automatic discovery makes the system user-friendly")
print("• Upload-and-forget experience like modern platforms")
print("• Natural language interface comes after data ingestion")

print("\n🚀 Ready for the Next Agent:")
print("Now that data is ingested, we can build the Planner Agent")
print("to convert natural language questions into execution plans!")

# =============================================
# CELL 14: Try It Yourself
# =============================================
print("\n💡 Try It Yourself!")
print("=" * 30)
print("1. Upload your own CSV files")
print("2. Create SQLite databases with multiple tables")
print("3. Try JSON files with complex document structures")
print("4. See how the agent handles different file sizes")

print("\n🔧 Understanding Check:")
print("• Why don't users provide SQL queries during ingestion?")
print("• How does automatic discovery work?")
print("• What happens to data from multiple tables?")
print("• When do natural language queries come into play?")
print("• How does this prepare data for other agents?")

print("\n🎯 Next Steps:")
print("• Master the upload-only user experience")
print("• Understand automatic data discovery")
print("• See how this prepares for natural language queries")
print("• Ready to build the Planner Agent!")